# Course Project - Practical Machine Learning
library(readr)
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url1)
testing <- read.csv(url2)
View(testing)
# lets check out the data
summary(training)
str(training)
head(training)
# we have to filter out some variables
grep("^accel", training, value = TRUE)
# we have to filter out some variables
grep("^accel", names(training), value = TRUE)
# we have to filter out some variables
# we are only interested in acceleration values
accel <- grep("^accel", names(training), value = TRUE)
training <- training[[names(training) == accel],]
training <- training[[names(training) == accel]]
training[["accel_belt_x"]]
training <- training[[accel]]
training <- training[[c(accel)]]
training[[accel]]
View(training)
training$classe
training[["accel_belt_x"]]
test <- training[[c("accel_belt_x", "classe")]]
training <- subset(training, select = c(accel, "classe"))
testing <- subset(testing, select = c(accel, "classe"))
testing <- subset(testing, select = accel)
# now some exploratory analysis
summary(training$accel_belt_x)
# now some exploratory analysis
library(ggplot2)
g <- ggplot(data = training, aes(x = accel_belt_x, y = accel_belt_y, colour = classe))
g + geom_point()
# check for NA
sum(is.na(training))
sum(is.na(testing))
training <- read.csv(url1)
testing <- read.csv(url2)
### First approach - using all predictors
library(caret)
fitAll <- train(classe ~ ., data = training, method = "rf")
sum(is.na(training$classe))
class(training$classe)
fitAll <- train(classe ~ ., data = training, method = "rf")
View(testing)
fitAll <- train(classe ~ ., data = training, method = "lda")
# Course Project - Practical Machine Learning
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url1)
testing <- read.csv(url2)
unique(training$classe)
### First approach - using all predictors
library(caret)
fitAll <- train(classe ~ ., data = training, method = "rf")
fitAll <- train(classe ~ ., data = training, method = "rf", na.action = na.exclude)
# we have to filter out some variables
# we are only interested in acceleration values
accel <- grep("^accel", names(training), value = TRUE)
training <- subset(training, select = c(accel, "classe"))
testing <- subset(testing, select = accel)
# check for NA
sum(is.na(training))
sum(is.na(testing))
# fit random forest model
modFit <- train(classe ~ ., data = training, method = "rf")
?caret
# fit random forest model
modFit <- train(classe ~ ., data = training, method = "adaboost")
# fit random forest model
modFit <- train(classe ~ ., data = training, method = "lda")
modFit
predict(modFit, newdata = training)
predTrain <- predict(modFit, newdata = training)
confusionMatrix(predTrain, training$classe)
# try support vector machine
library(e1071)
mod_svm <- svm(classe ~ ., data = training)
mod_svm
# check accuracy on test set
pred_svm <- predict(mod_svm)
confusionMatrix(pred_svm, training$classe)
# maybe we should try to leave out more predictors as the computation complexity is pretty high
# lets inspect the variability of the accelerations in the different dimensions
range(training)
# maybe we should try to leave out more predictors as the computation complexity is pretty high
# lets inspect the variability of the accelerations in the different dimensions
summary(training)
str(training)
# Split training set into training and test set.
# The original test set will be our validation set.
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
# Course Project - Practical Machine Learning
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url1)
testing <- read.csv(url2)
View(training)
### First approach - using all predictors
library(caret)
# Split training set into training and test set.
# The original test set will be our validation set.
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
training <- training[inTrain,]
training <- read.csv(url1)
validation <- read.csv(url2)
### First approach - using all predictors
library(caret)
# Split training set into training and test set.
# The original test set will be our validation set.
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
testing <- training[-inTrain,]
training <- training[inTrain,]
View(training)
#### Feature selection
# erase first 7 columns as these variables (name, time, window etc.)
# are not relevant for our prediction
training <- subset(training, select = -c(1:7))
testing <- subset(testing, select = -c(1:7))
View(validation)
?nearZeroVar
# nearZeroVar()
nearZeroVar(training)
# nearZeroVar()
nearZeroVar(training, saveMetrics = TRUE)
# nearZeroVar()
nzv <- nearZeroVar(training, saveMetrics = TRUE)
# nearZeroVar()
nzv <- nearZeroVar(training, saveMetrics = TRUE, names = TRUE)
nzv
which(nzv$nzv == TRUE)
nzvIndex <- which(nzv$nzv == TRUE)
training <- subset(training, select = -nzvIndex)
testing <- subset(testing, select = -nzvIndex)
nearZeroVar(training)
# lets check out the data
summary(training)
# theres still 97 predictors left.
# lets try lasso fitting and enet plot
mod_lasso <- train(classe ~ ., data = training, method = "lasso")
# theres still 97 predictors left.
# lets try lasso fitting and enet plot
mod_lasso <- train(classe ~ ., data = training, method = "lasso", na.action = na.roughfix)
# theres still 97 predictors left.
# lets try lasso fitting and enet plot
mod_lasso <- train(classe ~ ., data = training, method = "lasso", na.action = na.remove)
training <- read.csv(url1)
validation <- read.csv(url2)
#### Feature selection
# erase first 7 columns as these variables (name, time, window etc.)
# are not relevant for our prediction
training <- subset(training, select = -c(1:7))
View(training)
# search for near zero variance variables and omit them
nzv <- nearZeroVar(training, saveMetrics = TRUE, names = TRUE)
nzvIndex <- which(nzv$nzv == TRUE)
training <- subset(training, select = -nzvIndex)
# check if it worked
nearZeroVar(training)
# now remove NAS
for (variable in training){
sum(is.na(variable))
}
# now remove NAS
sum(is.na(training$roll_belt))
# now remove NAS
sum(is.na(training))
# now remove NAS
is.na(training)
# now remove NAS
isna <- is.na(training)
class(isna)
?is-na
?is.na
View(isna)
isna <- as.data.frame(isna)
View(isna)
which(sum(isna) / dim(isna$roll_belt)[1] > 0.5)
colSums(isna)
colSums(is.na(training))
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas
unique(colSums(is.na(training)))
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- which(sum(is.na(training)) > 0)
hasNA
# now remove variables with too many nas
colNA <- colSums(is.na(training))
unique(colNA)
colNA
class(coNA)
class(colNA)
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[, colSums(is.na(training)) > 0]
View(hasNA)
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[, - (colSums(is.na(training)) > 0)]
View(hasNA)
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[, - colNA > 0)]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[, - colNA > 0]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[, - (colNA > 0)]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[,  colNA = 0]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[,  which(colNA = 0)]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
hasNA <- training[,  colNA == 0]
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
training <- training[,  colNA == 0]
# now lets take a look at the correlation between the remaining variables
cor(training[, -"classe"])
# now lets take a look at the correlation between the remaining variables
cor(training[, -c("classe")])
training$classe
training[,"classe"]
training[,-"classe"]
training[,-("classe")]
# now lets take a look at the correlation between the remaining variables
cor(subset(training, select = -c("classe")))
# now lets take a look at the correlation between the remaining variables
cor(subset(training, select = -("classe")))
# now lets take a look at the correlation between the remaining variables
cor(subset(training, select = - "classe"))
# now lets take a look at the correlation between the remaining variables
cor(training)
# now lets take a look at the correlation between the remaining variables
cor(training[,-classe])
names(training)
training[,"classe"]
training[,-"classe"]
temp <- subset(training, select = -c(classe))
# now lets take a look at the correlation between the remaining variables
cor(training[,-c(classe)])
# now lets take a look at the correlation between the remaining variables
cor(subset(training, select = -c(classe)))
# now lets take a look at the correlation between the remaining variables
cor_mat <- cor(subset(training, select = -c(classe)))
# too many variables, lets try the findCorrelation function from the caret package
findCorrelation(cor_mat, cutoff = 0.9)
lower.tri(cor_mat)
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.9)
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.8)
colCor
length(colCor)
# remove those columns from data set
training <- training[, -colCor]
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.6)
colCor
length(colCor)
# theres still 40 predictors left.
# lets try lasso fitting and enet plot
mod_lasso <- train(classe ~ ., data = training, method = "lasso")
# theres still 40 predictors left.
# lets try lasso fitting and enet plot
mod_lasso <- train(classe ~ ., data = training, method = "rpart")
# theres still 40 predictors left.
# lets try lasso fitting and enet plot
library(elasticnet)
mod_lasso <- train(classe ~ ., data = training, method = "lasso")
class(training$classe)
training <- read.csv(url1)
validation <- read.csv(url2)
### First approach - using all predictors
library(caret)
#### Feature selection
# erase first 7 columns as these variables (name, time, window etc.)
# are not relevant for our prediction
training <- subset(training, select = -c(1:7))
# search for near zero variance variables and omit them
nzv <- nearZeroVar(training, saveMetrics = TRUE, names = TRUE)
nzvIndex <- which(nzv$nzv == TRUE)
training <- subset(training, select = -nzvIndex)
# check if it worked
nearZeroVar(training)
# now remove variables with too many nas
colNA <- colSums(is.na(training))
unique(colNA)
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
training <- training[,  colNA == 0]
# now lets take a look at the correlation between the remaining variables
cor_mat <- cor(subset(training, select = -c(classe)))
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.8)
# remove those columns from data set
training <- training[, -colCor]
# Split training set into training and test set.
# The original test set will be our validation set.
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
testing <- training[-inTrain,]
training <- training[inTrain,]
# theres still 40 predictors left.
# lets try lasso fitting and enet plot
library(elasticnet)
mod_lasso <- train(classe ~ ., data = training, method = "lasso")
# Lets try some model fitting
mod_lda <- train(classe ~ ., method = "lda", data = training)
pred_lda <- predict(mod_lda, newdata = testing)
mod_lda
confusionMatrix(pred_lda, testing$classe)$overall[1]
# try support vector machine
library(e1071)
mod_svm <- svm(classe ~ ., data = training)
mod_svm
# check accuracy on test set
pred_svm <- predict(mod_svm, newdata = testing)
confusionMatrix(pred_svm, training$classe)
confusionMatrix(pred_svm, testing$classe)
confusionMatrix(pred_svm, testing$classe)$overall[1]
# use svm model for validation set
pred_svm_validation <- predict(mod_svm, newdata = validation)
pred_svm_validation
View(validation)
validation <- subset(validation, select = -c(1:7))
validation <- subset(validation, select = -nzvIndex)
nearZeroVar(validation)
validation <- validation[, colNA == 0]
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.8)
validation <- validation[, -colCor]
# use svm model for validation set
pred_svm_validation <- predict(mod_svm, newdata = validation)
pred_svm_validation
View(training)
View(validation)
names(validation)
names(training)
pred_svm_validation
mod_lasso <- train(classe ~ ., data = training, method = "lasso")
class(training)
# now boosting (gbm)
mod_gbm <- train(classe ~ ., method = "gbm", data = training)
# decision tree
mod_tree <- train(classe ~ ., method = "rpart", data = training)
pred_tree <- predict(mod_tree, newdata = testing)
confusionMatrix(pred_tree, testing$classe)$overall[1]
mod_svm
confusionMatrix(pred_svm, testing$classe)$overall[1]
# Course Project - Practical Machine Learning
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url1)
validation <- read.csv(url2)
library(caret)
#### Feature selection
# erase first 7 columns as these variables (name, time, window etc.)
# are not relevant for our prediction
training <- subset(training, select = -c(1:7))
validation <- subset(validation, select = -c(1:7))
# search for near zero variance variables and omit them
nzv <- nearZeroVar(training, saveMetrics = TRUE, names = TRUE)
nzvIndex <- which(nzv$nzv == TRUE)
training <- subset(training, select = -nzvIndex)
validation <- subset(validation, select = -nzvIndex)
# check if it worked
nearZeroVar(training)
# now remove variables with too many nas
colNA <- colSums(is.na(training))
unique(colNA)
# as we can see, there are only columns with either
# 0 NA values or 19216 Nas.
# Lets remove those with the NAs
training <- training[,  colNA == 0]
validation <- validation[, colNA == 0]
# now lets take a look at the correlation between the remaining variables
cor_mat <- cor(subset(training, select = -c(classe)))
# too many variables, lets try the findCorrelation function from the caret package
colCor <- findCorrelation(cor_mat, cutoff = 0.8)
# remove those columns from data set
training <- training[, -colCor]
validation <- validation[, -colCor]
# Split training set into training and test set.
# The original test set will be our validation set.
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
testing <- training[-inTrain,]
training <- training[inTrain,]
# Lets try some model fitting
# first LDA:
mod_lda <- train(classe ~ ., method = "lda", data = training)
pred_lda <- predict(mod_lda, newdata = testing)
confusionMatrix(pred_lda, testing$classe)$overall[1]
# decision tree
mod_tree <- train(classe ~ ., method = "rpart", data = training)
pred_tree <- predict(mod_tree, newdata = testing)
confusionMatrix(pred_tree, testing$classe)$overall[1]
# try support vector machine
library(e1071)
mod_svm <- svm(classe ~ ., data = training)
#mod_svm
# check accuracy on test set
pred_svm <- predict(mod_svm, newdata = testing)
confusionMatrix(pred_svm, testing$classe)$overall[1]
# use svm model for validation set
pred_svm_validation <- predict(mod_svm, newdata = validation)
pred_svm_validation
# try bagging poor fitting models (lda and tree)
# first lda
# save predictors and outcome separately
predictors <- training[, c(1:length(training)-1)]
outcome <- training$classe
mod_lda_bag <- bag(predictors, outcome, B = 10,
bagControl = bagControl(fit = ldaBag$fit,
predict = ldaBag$pred,
aggregate = ldaBag$aggregate))
mod_lda_bag
pred_lda_bag <- predict(mod_lda_bag, newdata = testing)
confusionMatrix(pred_lda_bag, testing$classe)$overall[1]
# now try bagged decision tree
mod_tree_bag <- bag(predictors, outcome, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
pred_tree_bag <- predict(mod_tree_bag, newdata = testing)
confusionMatrix(pred_tree_bag, testing$classe)$overall[1]
# class of columns differ between training and validation.
# Lets fix this
classes_val <- sapply(validation, class)
classes_train <- sapply(training, class)
# find columns that have different types
result <- data.frame(classes_val, classes_train, stringsAsFactors = FALSE)
result[!(result$classes_val == result$classes_train), ]
# change magnet_dumbbell_z, magnet_forearm_y and magnet_forearm_z
# to numeric type in validation set
validation$magnet_dumbbell_z <- as.numeric(validation$magnet_dumbbell_z)
validation$magnet_forearm_y <- as.numeric(validation$magnet_forearm_y)
validation$magnet_forearm_z <- as.numeric(validation$magnet_forearm_z)
pred_tree_bag_validation <- predict(mod_tree_bag, newdata = validation)
pred_tree_bag_validation
# compare pred_tree_bag_validation and pred_svm_validation
table(pred_svm_validation, pred_tree_bag_validation)
